{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+5+3\n",
      "1+5+3\n",
      "2+5+3\n",
      "3+5+3\n",
      "4+5+3\n",
      "5+5+3\n",
      "[8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def add_func(a, b, c):\n",
    "    print(f\"{a}+{b}+{c}\")\n",
    "    return a + b + c\n",
    "\n",
    "\n",
    "add_list = list(map(partial(add_func, b=5, c=3), [0, 1, 2, 3, 4, 5]))\n",
    "\n",
    "print(add_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 64, 64))\n",
    "upsample = lambda x: torch.nn.functional.interpolate(x, scale_factor=2)\n",
    "up_x = upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"特徴量マップのための自己注意機構\"\"\"\n",
    "\n",
    "    def __init__(self, ch):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.ch = ch\n",
    "        self.theta = spectral_norm(\n",
    "            nn.Conv2d(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        )\n",
    "        self.phi = spectral_norm(\n",
    "            nn.Conv2d(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        )\n",
    "        self.g = spectral_norm(\n",
    "            nn.Conv2d(self.ch, self.ch // 2, kernel_size=1, padding=0, bias=False)\n",
    "        )\n",
    "        self.o = spectral_norm(\n",
    "            nn.Conv2d(self.ch // 2, self.ch, kernel_size=1, padding=0, bias=False)\n",
    "        )\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # apply convs\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), [2, 2])\n",
    "        g = F.max_pool2d(self.g(x), [2, 2])\n",
    "        # perform reshapes\n",
    "        theta = theta.view(-1, self.ch // 8, x.shape[2] * x.shape[3])\n",
    "        phi = phi.view(-1, self.ch // 8, x.shape[2] * x.shape[3] // 4)\n",
    "        g = g.view(-1, self.ch // 2, x.shape[2] * x.shape[3] // 4)\n",
    "        # matmul and softmax to get attention maps\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)\n",
    "        # Attention map times g path\n",
    "        o = self.o(\n",
    "            torch.bmm(g, beta.transpose(1, 2)).view(\n",
    "                -1, self.ch // 2, x.shape[2], x.shape[3]\n",
    "            )\n",
    "        )\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfAttention(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock_UP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ResBlock_UP, self).__init__()\n",
    "\n",
    "        self.sn_conv0 = spectral_norm(nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=True))\n",
    "        self.sn_conv1 = spectral_norm(nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=True))\n",
    "        self.sn_conv_sc = spectral_norm(nn.Conv2d(in_ch, out_ch, 1, 1, 0, bias=True))\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(in_ch)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.upsample = lambda x: torch.nn.functional.interpolate(x, scale_factor=2)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "\n",
    "    def residual(self, z, y):\n",
    "\n",
    "        h = self.bn0(z, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.upsample(h)\n",
    "        h = self.sn_conv0(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.activation(h)\n",
    "        h = self.sn_conv1(h)\n",
    "        return h\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.upsample(x)\n",
    "            x = self.sn_conv_sc(x)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def foward(self, z, y):\n",
    "        return self.residual(z, y) + self.shortcut(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1024, 512, 512, 256, 128, 64]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = \"128\"\n",
    "ch = 64\n",
    "{2 ** i: (2 ** i in [int(item) for item in attention.split(\"_\")]) for i in range(3, 9)}\n",
    "[ch * item for item in [16, 8, 8, 4, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, ch=64, dim_z=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.sn_linear = spectral_norm(nn.Linear(dim_z, 4 * 4 * 16 * ch, bias=True))\n",
    "        self.res0 = ResBlock_UP(16 * ch, 16 * ch)\n",
    "        self.res1 = ResBlock_UP(16 * ch, 8 * ch)\n",
    "        self.res2 = ResBlock_UP(8 * ch, 8 * ch)\n",
    "        self.res3 = ResBlock_UP(8 * ch, 4 * ch)\n",
    "        self.res4 = ResBlock_UP(4 * ch, 2 * ch)\n",
    "        self.attn = SelfAttention(2 * ch)\n",
    "        self.res5 = ResBlock_UP(2 * ch, ch)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.ReLU(inplace=False),\n",
    "            spectral_norm(nn.Conv2d(ch, 3, bias=True)),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,ngpu):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "    def forward(self,input):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.utils.spectral_norm(nn.Linear(20, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=40, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
